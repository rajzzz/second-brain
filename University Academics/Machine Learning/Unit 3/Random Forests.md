A Random Forest is an [[Ensemble Learning]] method that combines multiple [[Decision Tree]]s to improve the performance.

It is based on the principle of [[Bagging]] and [[Random Feature Selection]].

#### Why Random Forest?
- A single Decision Tree is **prone to overfitting**.
- Random Forest **reduces variance** by averaging predictions over many trees.
- It handles both [[Classification]] and [Regression](Regression-Analysis) tasks very well.

### Advantages:
- Handles high-dimensional data
- Robust to outliers and noise
- Less prone to overfitting than a single Decision Tree